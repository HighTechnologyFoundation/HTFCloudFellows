{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Global Forecast System (GFS) data and simple visualization\n",
    "\n",
    "### References:\n",
    "- https://www.nco.ncep.noaa.gov/pmb/products/gfs/#GFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function `file_path()` to fetch the urls in public Azure container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell to install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install --upgrade xarray[complete]\n",
    "# %pip install eccodes\n",
    "# %pip install cfgrib\n",
    "# %pip install ecmwflibs\n",
    "# %pip install numpy==1.23.0\n",
    "%pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def file_path_JL(cycle_runtime: str, forecast_hour: str, year: int, month: str, day: str, resolution_degree: float) -> str:\n",
    "    prefix_path = \"https://noaagfs.blob.core.windows.net/\"\n",
    "    product_name = \"gfs\"\n",
    "\n",
    "    if len(cycle_runtime) == 1:\n",
    "        cycle_runtime = cycle_runtime.rjust(2, \"0\")\n",
    "\n",
    "    if len(forecast_hour) != 3:\n",
    "        forecast_hour = forecast_hour.rjust(3, \"0\")\n",
    "\n",
    "    if len(month) == 1:\n",
    "        month = month.rjust(2, \"0\")\n",
    "\n",
    "    if len(day) == 1:\n",
    "        day = day.rjust(2, \"0\")\n",
    "\n",
    "    if len(str(resolution_degree).split(\".\")[1]) == 1:\n",
    "        split_resolution_degree = []\n",
    "        split_resolution_degree.append(str(resolution_degree).split(\".\")[0])\n",
    "        split_resolution_degree.append(str(resolution_degree).split(\".\")[1].ljust(2, \"0\"))\n",
    "    else:\n",
    "        split_resolution_degree = str(resolution_degree).split(\".\")\n",
    "\n",
    "    file_path = (\n",
    "        f\"{product_name}/{product_name}.{year}{month}{day}/\"\n",
    "        f\"{cycle_runtime}/atmos/{product_name}.t{cycle_runtime}z.\"\n",
    "        f\"pgrb2.{split_resolution_degree[0]}p{split_resolution_degree[1]}.f{forecast_hour}\"\n",
    "    )\n",
    "\n",
    "    whole_path = os.path.join(prefix_path, file_path)\n",
    "\n",
    "    return whole_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# KJW: make the parameters have consistent types.  e.g. month should be an int, not a string\n",
    "def file_path(cycle_runtime: int, forecast_hour: int, year: int, month: int, day: int, resolution_degree: float) -> str:\n",
    "    prefix_path = \"https://noaagfs.blob.core.windows.net/\"\n",
    "    product_name = \"gfs\"\n",
    "\n",
    "    resolution_split = str(resolution_degree).split(\".\")\n",
    "\n",
    "    file_path = (\n",
    "        f\"{product_name}/{product_name}.{year}{month:>02}{day:>02}/\"\n",
    "        f\"{cycle_runtime:>02}/atmos/{product_name}.t{cycle_runtime:>02}z.\"\n",
    "        f\"pgrb2.{resolution_split[0]}p{resolution_split[1]:<02}.f{forecast_hour:>03}\"\n",
    "    )\n",
    "\n",
    "    whole_path = os.path.join(prefix_path, file_path)\n",
    "\n",
    "    return whole_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function `read_into_xarray_dataset()` to read given url into xarray dataset\n",
    "\n",
    "References for different keywords:\n",
    "\n",
    "        filter_by_keys={'typeOfLevel': 'meanSea'}\n",
    "        filter_by_keys={'typeOfLevel': 'hybrid'}\n",
    "        filter_by_keys={'typeOfLevel': 'atmosphere', 'steptype': 'instant'}\n",
    "        filter_by_keys={'typeOfLevel': 'atmosphere', 'steptype': 'avg'}\n",
    "        filter_by_keys={'typeOfLevel': 'surface', 'steptype': 'instant'}\n",
    "        filter_by_keys={'typeOfLevel': 'surface', 'steptype': 'avg'}\n",
    "        filter_by_keys={'typeOfLevel': 'surface', 'steptype': 'accum'}\n",
    "        filter_by_keys={'typeOfLevel': 'planetaryBoundaryLayer'}\n",
    "        filter_by_keys={'typeOfLevel': 'isobaricInPa'}\n",
    "        filter_by_keys={'typeOfLevel': 'isobaricInhPa'}\n",
    "        filter_by_keys={'typeOfLevel': 'heightAboveGround'}\n",
    "        filter_by_keys={'typeOfLevel': 'depthBelowLandLayer'}\n",
    "        filter_by_keys={'typeOfLevel': 'heightAboveSea'}\n",
    "        filter_by_keys={'typeOfLevel': 'atmosphereSingleLayer'}\n",
    "        filter_by_keys={'typeOfLevel': 'lowCloudLayer', 'steptype': 'instant'}\n",
    "        filter_by_keys={'typeOfLevel': 'lowCloudLayer', 'steptype': 'avg'}\n",
    "        filter_by_keys={'typeOfLevel': 'middleCloudLayer', 'steptype': 'instant'}\n",
    "        filter_by_keys={'typeOfLevel': 'middleCloudLayer', 'steptype': 'avg'}\n",
    "        filter_by_keys={'typeOfLevel': 'highCloudLayer', 'steptype': 'instant'}\n",
    "        filter_by_keys={'typeOfLevel': 'highCloudLayer', 'steptype': 'avg'}\n",
    "        filter_by_keys={'typeOfLevel': 'cloudCeiling'}\n",
    "        filter_by_keys={'typeOfLevel': 'heightAboveGroundLayer'}\n",
    "        filter_by_keys={'typeOfLevel': 'tropopause'}\n",
    "        filter_by_keys={'typeOfLevel': 'maxWind'}\n",
    "        filter_by_keys={'typeOfLevel': 'isothermZero'}\n",
    "        filter_by_keys={'typeOfLevel': 'highestTroposphericFreezing'}\n",
    "        filter_by_keys={'typeOfLevel': 'pressureFromGroundLayer'}\n",
    "        filter_by_keys={'typeOfLevel': 'sigmaLayer'}\n",
    "        filter_by_keys={'typeOfLevel': 'sigma'}\n",
    "        filter_by_keys={'typeOfLevel': 'potentialVorticity'}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def read_into_xarray_dataset(URL: str, level: str, step: Optional[str] = None):\n",
    "    try:\n",
    "        filename, _ = urllib.request.urlretrieve(URL)\n",
    "\n",
    "        step_key = [\"atmosphere\", \"surface\", \"lowCloudLayer\", \"middleCloudLayer\", \"highCloudLayer\"]\n",
    "\n",
    "        if level in step_key:\n",
    "            ds = xr.open_dataset(\n",
    "                filename,\n",
    "                engine=\"cfgrib\",\n",
    "                filter_by_keys={\"typeOfLevel\": level, \"stepType\": step},\n",
    "                backend_kwargs={\"errors\": \"ignore\"},\n",
    "            )\n",
    "        else:\n",
    "            ds = xr.open_dataset(\n",
    "                filename, \n",
    "                engine=\"cfgrib\", \n",
    "                filter_by_keys={\"typeOfLevel\": level}, \n",
    "                backend_kwargs={\"errors\": \"ignore\"}\n",
    "            )\n",
    "\n",
    "        return ds\n",
    "    except HTTPError as err:\n",
    "        if err.code == 404:\n",
    "            print(f\"{URL} does not exist. Please check the parameters again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = file_path(cycle_runtime=12, forecast_hour=102, year=2024, month=6, day=10, resolution_degree=0.5)\n",
    "ds = read_into_xarray_dataset(URL, 'surface', 'accum')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple visualization of gfs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "# forecast_hours = [str(x) for x in range(12, 72 + 1, 12)]\n",
    "forecast_hours = [x for x in range(6, 72 + 1, 6)]\n",
    "fig = plt.subplots(figsize = (15, 12))\n",
    "\n",
    "rows = len(forecast_hours) // 2 + len(forecast_hours) % 2\n",
    "print(rows)\n",
    "cols = 2\n",
    "\n",
    "with alive_bar(len(forecast_hours), force_tty=True, title='Running', length=20, bar = 'smooth') as bar:\n",
    "    \n",
    "    for n, forecast_hour in enumerate(forecast_hours):\n",
    "        \n",
    "        URL = file_path(12, forecast_hour, 2024, 6, 12, 1.)\n",
    "\n",
    "        ds = read_into_xarray_dataset(URL, 'pressureFromGroundLayer')\n",
    "\n",
    "        ax = plt.subplot(rows, cols, n + 1)\n",
    "        \n",
    "        ds['t'].plot(ax = ax)\n",
    "\n",
    "        plt.title(f\"forecast {ds.step.values.astype('timedelta64[h]')} from {ds.time.values.astype('datetime64[s]')}\")  \n",
    "\n",
    "        bar()\n",
    "\n",
    "plt.suptitle('Three days forecast of Temperature with 1.00 degree resolution', size = 18)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.92)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8), subplot_kw={'projection': ccrs.PlateCarree(central_longitude=180)})\n",
    "ds['r'].plot(ax = ax)\n",
    "plt.title(f\"forecast {ds.step.values.astype('timedelta64[h]')} from {ds.time.values.astype('datetime64[s]')}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
