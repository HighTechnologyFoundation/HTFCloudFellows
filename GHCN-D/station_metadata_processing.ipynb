{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f34b8a41-c51a-42d8-ba68-2a22053bf7de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create, enrich, and clean station metadata from three different sources:\n",
    "- https://cloford.com/resources/codes/index.htm\n",
    "- https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\n",
    "- https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-countries.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f1b0b41-1ffc-4d98-963a-0f815ac3030b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1. scrape data from [https://cloford.com/resources/codes/index.htm](https://cloford.com/resources/codes/index.htm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6eb0e9c-cd1a-47e1-ae9b-8dfa5cb83705",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e824e59-3462-45fb-ae30-b49a7d2e50d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def translate_table_to_dataframe(table):\n",
    "    dataframe_friendly_array = []\n",
    "    dataframe_ready_headers = []\n",
    "    column_headers = table.find_all('th')\n",
    "    for header in column_headers:\n",
    "        dataframe_ready_headers.append(header.text)\n",
    "\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    for row in rows:\n",
    "        row_data = []\n",
    "        for col in row.find_all('td'):\n",
    "            try:\n",
    "                row_data.append(col.text)\n",
    "            except:\n",
    "                continue\n",
    "        dataframe_friendly_array.append(row_data)\n",
    "            \n",
    "    dataframe = pd.DataFrame(data = dataframe_friendly_array, columns = dataframe_ready_headers)\n",
    "    \n",
    "    # we drop index 0 because it is always a set of null values\n",
    "    return dataframe.drop(index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b92ff532-3827-45ff-b368-c2430ad0b498",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_of_dataframes = []\n",
    "\n",
    "# get hhe web page with the data\n",
    "page = requests.get(\"https://cloford.com/resources/codes/index.htm\")\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# locate the tables within the page\n",
    "tables = soup.find_all('table',class_='outlinetable')\n",
    "print(f'There were {len(tables)} tables found:\\n')\n",
    "\n",
    "# merged the table into one dataframe\n",
    "merged_tables_df = pd.DataFrame()\n",
    "\n",
    "for each_table in tables:\n",
    "    df = translate_table_to_dataframe(each_table)\n",
    "    if merged_tables_df is None:\n",
    "        merged_tables_df = df\n",
    "    else:\n",
    "        merged_tables_df = pd.concat([merged_tables_df, df])\n",
    "\n",
    "    list_of_dataframes.append(df)\n",
    "\n",
    "names = [\"Country Codes\", \"Additional Codes\", \"Additional FIPS 10-4 Codes\"]\n",
    "\n",
    "for one_dataframe, one_name in zip(list_of_dataframes, names):\n",
    "    one_dataframe.name = one_name\n",
    "    print(one_dataframe.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea7a50c9-1370-4596-b909-4fe8d2389a56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# replace nonsense values with 'No Record'\n",
    "remove_list = ['-', '--', '\\xa0', np.nan]\n",
    "\n",
    "for column in merged_tables_df.columns:\n",
    "    merged_tables_df.loc[merged_tables_df[column].isin(remove_list), column] = 'No Record'\n",
    "\n",
    "# Assign the value of column 'ISO (2)' and 'Internet' for country 'Namibia' to be 'NA'\n",
    "merged_tables_df.loc[merged_tables_df.Country == 'Namibia', ['ISO (2)', 'Internet']] = 'NA'\n",
    "\n",
    "# Make sure there is no null values in any column\n",
    "merged_tables_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e058238-6eab-4b05-9bb6-16a4e6797c6c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2. process ghcnd-stations text file from URL https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65b63339-17ab-4b5c-8be0-b5ecd7c743df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# get all the staion id information\n",
    "URL_station = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt\"\n",
    "\n",
    "# processing text by splitting specific range of characters into meaningful list\n",
    "with urllib.request.urlopen(URL_station) as file:\n",
    "    lines = [line.decode('utf-8').rstrip() for line in file]\n",
    " \n",
    "    ID_list = [line[:11] for line in lines]\n",
    " \n",
    "    lat_list = [line[12:20].lstrip() for line in lines]\n",
    " \n",
    "    lon_list = [line[21:30].lstrip() for line in lines]\n",
    "   \n",
    "    state_list = [line[38:40].lstrip() for line in lines]\n",
    " \n",
    "    station_name_list = [line[41:71].strip() for line in lines]\n",
    "\n",
    "# create a dataframe with the lists we just created \n",
    "metadata_station = pd.DataFrame({  \n",
    "    'ID' : ID_list,\n",
    "    'Latitude' : lat_list,\n",
    "    'Longitude' : lon_list,\n",
    "    'State' : state_list,\n",
    "    'StationName' : station_name_list\n",
    "})\n",
    "\n",
    "# replace blanks with nan value\n",
    "metadata_station['State'] = metadata_station['State'].replace('', np.nan)\n",
    "\n",
    "# create new column 'FIPS' by extracting the first two characters of 'ID' column\n",
    "metadata_station['FIPS'] = metadata_station.ID.str[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a83b1bf-88d6-495e-9632-d4d244be6564",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. process ghcnd-countries text file from URL https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-countries.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e8cf6e-a7d1-49f3-9479-0510fdf0d1aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get all the country codes\n",
    "URL_countries = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-countries.txt\"\n",
    "\n",
    "with urllib.request.urlopen(URL_countries) as file:\n",
    "    lines = [line.decode(\"utf-8\").rstrip().split(\" \", 1) for line in file]\n",
    "\n",
    "metadata_countries = pd.DataFrame(lines, columns=['FIPS', 'Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d9f6154-c3d7-4305-a6a1-2fc2e2fef14f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. merge all three dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cec5462a-265d-4d08-ba5d-223a1a89fe61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicated column 'Country' \n",
    "merged_tables_df.drop('Country', axis=1, inplace = True)\n",
    "\n",
    "# merge three dataframes based on the 'FIPS' column\n",
    "middle_metadata = pd.merge(metadata_station, metadata_countries, on = 'FIPS')\n",
    "final_metadata = pd.merge(middle_metadata, merged_tables_df, on = 'FIPS', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e78b81c1-321f-4c21-8193-f22c46066597",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4. perform data cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90079a6f-c0cc-4d61-a0df-de8c3f06e1f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# assign missing continent and region value to specific FIPS\n",
    "# values are hard-coded and searched on google and wikipedia\n",
    "missing_data_fips_list = final_metadata[final_metadata.Region.isnull() == True][\"FIPS\"].unique()\n",
    "missing_data_continent_list = [\"Asia\", \"Atlantic Ocean\", \"Oceania\", \"Europe\", \"Americas\", \"Europe\", \"Americas\"]\n",
    "missing_data_region_list = [\n",
    "    \"South West Asia\",\n",
    "    \"South Atlantic Ocean\",\n",
    "    \"North Pacific Ocean\",\n",
    "    \"South East Europe\",\n",
    "    \"North America\",\n",
    "    \"South East Europe\",\n",
    "    \"South America\",\n",
    "]\n",
    "missing_data_dict = {\n",
    "    fips: [missing_continent, missing_region]\n",
    "    for fips, missing_continent, missing_region in zip(\n",
    "        missing_data_fips_list, missing_data_continent_list, missing_data_region_list\n",
    "    )\n",
    "}\n",
    "\n",
    "for fips, missing_data in missing_data_dict.items():\n",
    "    final_metadata.loc[final_metadata[\"FIPS\"] == fips, \"Continent\"] = missing_data[0]\n",
    "\n",
    "    final_metadata.loc[final_metadata[\"FIPS\"] == fips, \"Region\"] = missing_data[1]\n",
    "\n",
    "# replace null value in 'State' column with 'Out of States'\n",
    "final_metadata.loc[final_metadata[\"State\"].isna(), \"State\"] = \"Out of States\"\n",
    "# assign Capital value of FIPS equals to 'AE' to be 'Abu Dhabi'\n",
    "final_metadata.loc[final_metadata[\"FIPS\"] == \"AE\", \"Capital\"] = \"Abu Dhabi\"\n",
    "# rename column format so that databricks environment can recognize\n",
    "final_metadata.rename({\"ISO (2)\": \"ISO-2\", \"ISO (3)\": \"ISO-3\", \"ISO (No)\": \"ISO-No\"}, axis=1, inplace=True)\n",
    "# replace null values with 'No Record'\n",
    "final_metadata.fillna(\"No Record\", inplace=True)\n",
    "\n",
    "# Make sure there is no null values in any column\n",
    "final_metadata.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33a6cbb3-5fab-4bf3-8079-307be3a9641c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 5. create schema `ghcn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d908a413-d9b4-4af5-b989-7082a6b17898",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS ghcn;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fb9a35d-a87f-46a6-b999-cbee18e9f91a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 6. write the final result into `ghcn.station_metadata` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b59428d9-9905-4f2b-9cca-bc6c3cec8257",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(final_metadata).write.mode(\"overwrite\").saveAsTable(\"ghcn.station_metadata\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2745134426726060,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "station_metadata_processing",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
