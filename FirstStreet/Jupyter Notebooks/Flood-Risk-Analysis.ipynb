{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bcedf7-755f-4194-befe-e690f28d9248",
   "metadata": {},
   "source": [
    "# Example of student assignment for Climate Risk Analysis\n",
    "## Getting students exposed to climate risk data and performing analysis to extract insight from it.\n",
    "- The original dataset utilizes a sophisticated model to quantify the risk of four different hazards (flood, fire, heat, wind) across the United States, broken down into national, state, county, sub-county, and tract layers. \n",
    "- This assignment involves the practice of data access, data manipulation, data enrichment, data visualization, and fundamental geospatial analysis.\n",
    "- It has a wide range of complexity of data usage. Instructors are welcome to challenge students to think of new research questions and perform the analysis to find solutions.\n",
    "## There are two tasks illustrated in this example:\n",
    "1. What are the top five states that have the highest flood risk?\n",
    "2. What is the geospatial distribution of flood risk across the sub-counties in West Virginia?\n",
    "# Prerequisites\n",
    "This notebook assumes the following:\n",
    "- A general understanding of the model and methodology behind the datasets.  See https://firststreet.org/methodology/\n",
    "\n",
    "    - The core of the First Street Foundation Flood Model (FSF-FM) is built upon a complex of hydraulic and hydrology models. Earth and climate projection data seek to account for the cause and effect of inland and coastal floodings. Probabilistic flooding scenarios from climate projection analysis are established and ingested into the FSF-FM to produce realistic flood hazard layers for the current and future. The FSF-FM mainly consists of four major components: \n",
    "        * inland (e.g., pluvial and fluvial) flood modeling\n",
    "        * coastal flood modeling\n",
    "        * computing (flood model execution)\n",
    "        * post-processing.\n",
    "- Subscribe to the dataset on AWS to get access: https://aws.amazon.com/marketplace/seller-profile?id=b777a8d0-ad41-4190-b94a-27e18e87e17f.  This process is explained in the ```How to access data on AWS.docx``` document.\n",
    "    - The flood data has been downloaded to your local machine in the directory `..\\Climate Risk Data\\Flood-Risk-Data`.\n",
    "        Note: If this is not the location of your data, modify the `dataDirectory` variable below.\n",
    "- Obtain auxiliary data source for geospatial information: https://catalog.data.gov/dataset/tiger-line-shapefile-2019-state-west-virginia-current-county-subdivision-state-based\n",
    "    - Retrieve the \"TIGER/Line Shapefile, 2019, state, West Virginia, Current County Subdivision State-based\" dataset. The TIGER/Line shapefiles and related database files (.dbf) are an extract of selected geographic and cartographic information from the U.S. Census Bureau's Master Address File / Topologically Integrated Geographic Encoding and Referencing (MAF/TIGER) Database (MTDB).\n",
    "    - Download this dataset as a zip file and extract it into the directory `..\\Climate Risk data\\Auxiliary Data`\n",
    "        Note: If this is not the location of your data, modify the `auxilliaryDataDirectory` variable below.\n",
    "\n",
    "- The following packages need to be installed into your python environment:\n",
    "    * pandas\n",
    "    * geopandas\n",
    "    * matplotlib\n",
    "    * seaborn\n",
    "    * contextily\n",
    "    * folium\n",
    "    * mapclassify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f28585",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1\n",
    "### Read data from data source on the local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c2d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the locations of the data files9\n",
    "dataDirectory = \"..\\\\Climate Risk data\\\\Flood-Risk-Data\\\\\"\n",
    "auxilliaryDataDirectory = \"..\\\\Climate Risk data\\\\Auxiliary Data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pandas library and read the data\n",
    "import pandas as pd\n",
    "\n",
    "stateFloodSummary = pd.read_csv(f\"{dataDirectory}fsf_flood_state_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b2595",
   "metadata": {},
   "source": [
    "### Quick exploratory data analysis\n",
    "Explore the data to gain some familiarity with its structure and format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3493096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateFloodSummary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162cd3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateFloodSummary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateFloodSummary.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdca413",
   "metadata": {},
   "outputs": [],
   "source": [
    "stateFloodSummary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e558e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the number of missing values in each column\n",
    "stateFloodSummary.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22a5cdd",
   "metadata": {},
   "source": [
    "### Data manipulation and preprocessing\n",
    "\n",
    "The original dataset quantifies the climate risk by assigning each properties within the geographical unit to one risk factor from 1 - 10, with 10 the most severe. In order to simplify the calculation, we create a new variable to derive the weighted average risk by adding up the number of properties times factor index, then divided by the total number of properties, in this case 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deriving the weighted average for each state because our weights match the floodfactor indexes\n",
    "stateFloodSummary['average_risk'] = 0\n",
    "for i in range (1,11):\n",
    "    stateFloodSummary['average_risk'] += stateFloodSummary[f'count_floodfactor{i}'] * i\n",
    "stateFloodSummary['average_risk'] /= stateFloodSummary['count_property']\n",
    "\n",
    "# Sort the data by average risk from the highest to the lowest\n",
    "sorted_data = stateFloodSummary.sort_values(by='average_risk', ascending = False)\n",
    "\n",
    "# Display the top 10 states with the highest average risk\n",
    "sorted_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd30071",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "Utilize the visualization packages ```matplotlib``` and ```seaborn``` to present bar charts that show the top five states that have the highest flood risk with the sorted data we just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b4551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#create bar chart to show the top five states that have the highest flood risk. \n",
    "plt.figure(figsize=(8,5))\n",
    "palette = sns.color_palette(\"Blues\", as_cmap = True)\n",
    "ax = sns.barplot(data = sorted_data.head(), \n",
    "                 x = 'name', \n",
    "                 y = 'average_risk', \n",
    "                 hue=\"average_risk\", \n",
    "                 palette = palette, \n",
    "                 legend = False)\n",
    "plt.title('Top five States with highest flood risk')\n",
    "ax.set(xlabel=None)\n",
    "plt.tight_layout()\n",
    "\n",
    "# This will save the plot as a .png file\n",
    "# plt.savefig(\"Top Five States with highest flood risk.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7467347",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2\n",
    "\n",
    "### Read the subcounty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae46bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "floodSummary = pd.read_csv(f\"{dataDirectory}fsf_flood_cousub_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7acd6",
   "metadata": {},
   "source": [
    "### Quick exploratory data analysis\n",
    "Explore the data to gain some familiarity with its structure and format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf9cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "floodSummary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af778060",
   "metadata": {},
   "outputs": [],
   "source": [
    "floodSummary.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b664390",
   "metadata": {},
   "outputs": [],
   "source": [
    "floodSummary.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac3bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "floodSummary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "floodSummary.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82641c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows that have a null value in them\n",
    "floodSummary = floodSummary.dropna()\n",
    "floodSummary.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da2228e",
   "metadata": {},
   "source": [
    "### Data manipulation and enrichment\n",
    "\n",
    "Data enrichment can be extremely critical and valuable. It focuses specifically on the addition of new and supplemental information to existing datasets. In this notebook, since we would like to visualize the risk distribution on the map, we need the geospatial information (\"geometry\") to do so. Therefore, we downloaded \"tl_2019_54_cousub.shp\" file, read it into geopandas, and merged it with risk data on the same FIPS column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2bdcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the geopandas library to deal with GIS shape data\n",
    "import geopandas as gpd\n",
    "\n",
    "# Read data that has the geospatial information for sub-county areas in West Virginia\n",
    "geospatialData = gpd.read_file(f\"{auxilliaryDataDirectory}tl_2019_54_cousub.shp\")\n",
    "geospatialData = geospatialData.to_crs(\"EPSG:4326\")\n",
    "geospatialData = geospatialData.rename(columns = {'GEOID':'fips'})\n",
    "\n",
    "# Display the first few rows of the geospatial data\n",
    "geospatialData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from our main dataset that is for West Virginia \n",
    "floodSummary.fips = floodSummary['fips'].astype(str)\n",
    "floodSummary = floodSummary[floodSummary.fips.str.startswith('54')]  # 54 is the FIPS code for West Virginia\n",
    "\n",
    "# Calculate the weighted average risk for each sub-county area in WV\n",
    "floodSummary['average_risk'] = 0\n",
    "for i in range (1,11):\n",
    "    floodSummary['average_risk'] += floodSummary[f'count_floodfactor{i}'] * i\n",
    "floodSummary['average_risk'] /= floodSummary['count_property']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d16ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge both datasets with the same column \"fips\"\n",
    "result = pd.merge(floodSummary, geospatialData, on = 'fips')\n",
    "\n",
    "# Extract the columns we need to display the final data\n",
    "final_data = result[['fips', 'name', 'average_risk', 'geometry']]\n",
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e52d6",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "Knowing the standard of the data that you're planning to use is important, as different mapping services operate on different coordinate reference systems (CRS). EPSG:4326 is a popular standard CRS based on the WGS84 projection.\n",
    "   - More information about the CRS: https://8thlight.com/insights/geographic-coordinate-systems-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d272b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geodataframe preparation\n",
    "crs = {'init':'EPSG:4326'} \n",
    "geo_df = gpd.GeoDataFrame(final_data, crs = crs, geometry = final_data.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show an interactive map\n",
    "geo_df.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50640e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the flood risks\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "geo_df.plot(column = 'average_risk', ax = ax, cmap = 'Blues',\n",
    "            legend = True, legend_kwds={'shrink': 0.5, 'label':'Risk'}, \n",
    "            markersize = 10)\n",
    "ax.set_title('West Virginia subcounties Flood Risk')\n",
    "plt.show()\n",
    "\n",
    "# This will save the plot as a .png file\n",
    "#plt.savefig('WV_cousub_floodrisk.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f2720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data on top of a map\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "df_wm = geo_df.to_crs(epsg=3857)\n",
    "if df_wm is not None:\n",
    "    df_wm.plot(column = 'average_risk',\n",
    "               ax = ax, \n",
    "               cmap = 'Blues',\n",
    "               legend = True, \n",
    "               legend_kwds={'shrink': 0.5, 'label':'Risk'}, \n",
    "               markersize = 10)\n",
    "    ax.set_title('West Virginia subcounties Flood Risk')\n",
    "\n",
    "# Import contextily to be able to import an extenal map source\n",
    "import contextily as ctx\n",
    "\n",
    "# Add a background map to the plot\n",
    "ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905a3dc5",
   "metadata": {},
   "source": [
    "## There are many other potential analyses for students to explore and discover!\n",
    "......"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
